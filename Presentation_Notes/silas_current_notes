Technologies: 
In order to efficiently achieve our goal of identifying past and predicting future Hall of Fame players, we used this full list of technologies.
•	Python and Pandas, which were used for data exploration and ETL
•	PostgreSQL was used for a database
•	Extreme Gradient or XGBoost classifier with Smote, Random Forest Classifier, and Logistic Regression were all used for Machine Learning
•	Matplotlib, and Tableau for visualizing our findings
•	Heroku, Flask, HTML, and JavaScript for dashboard creation and deployment
ETL(Extract, Transform, Load):
•	ETL was one of our earlier steps we took. This helped us clean up our databases to make them more useful for our Machine Learning model. 
•	When we started making decisions about how we would want our predictions to run we decided to split the data frame up between batters and pitchers. 
•	Both batters and pitchers CSV’s came from Sean Lahman’s baseball database.
•	NEXT SLIDE
•	We created new data frames by reading in the Hall of Fame data into our batters and pitchers CSV files.
•	This allowed up to make the appropriate filters while adding additional columns that were not in the original df.
•	For example, in our batting we needed to add key stats such as batting average, on base percentage, and slugging percentage.
•	While in our pitching we added stats such as win percentage, strikeout/walk ratio, walks/inning pitched and earned run average.
•	All these additions were essential stats that were needed in determining what a Hall of Fame player is.
•	We did have a couple of issues that needed addressed when creating both data frames. 
•	In the batter data frame, we were making our filters too narrow at first and cutting out too many players
•	And in the pitching df we noticed that the earned run average on the CSV was not correct, so we had to recalculate it
•	A note to remember is that the MLB is currently working to integrate the Negro League stats from 1920-1948 which will increase our data frame

